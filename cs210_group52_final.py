# -*- coding: utf-8 -*-
"""CS210_group52_FINAL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1csyS-GEgt1nvBQOz9YtMDPIdM5ptXk76

# [NEW YORK AIRBNB PRICES - GROUP 52]

Group Members:

Bengisu Özdemir

Oğuzhan Kaya

Sarp Bora Polat

## Introduction

<font color="black">
The purpose of this project is to analyze the given NYC Airbnb data and establishing a connection between the main data set and our additional data sets. In terms of additional data sets, we are using NYC Subway data, NYC Bus Stop data, and NYC Point of Interests. By making these we are observing the connections between transportation opportunities and price points, whether a location's price increase/decrease depending on their distance to POI's. In order to do so, we visualized our data via bar graphs, word clouds, and tables.

</font>

### Utilized Datasets

We have used several datasets from Kaggle.

The main data is "New York City Airbnb Open Data" and it's link: https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data

There are 3 more datasets we used throught the project as a side data for the main data.

Here additional 3 data set's links:

"A List of Important POIs"
https://data.cityofnewyork.us/Health/Places/mzbd-kucq

"Bus Stop Locations"
https://data.cityofnewyork.us/Transportation/Bus-Stop-Shelters/qafz-7myz

"Subway Locations"
https://data.cityofnewyork.us/Transportation/Subway-Stations/arq3-7z49
"""

from google.colab import drive
drive.mount("./drive")
path_prefix = "./drive/My Drive"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from os.path import join
from scipy.stats import f_oneway
import seaborn as sns
from scipy import stats
# %matplotlib inline

filename_first_data = "AB_NYC_2019.csv"
df = pd.read_csv(join(path_prefix, filename_first_data))

df.head()
#main

df.dtypes

df.isna().sum()

df.describe()

filename_second_data = "Areas_of_Interest_Centroids (1).csv"
dataframe2 = pd.read_csv(join(path_prefix, filename_second_data))

dataframe2.head()
#POI

dataframe2.isna().sum()

filename_third_data = "Bus_Stop_Shelter.csv"
dataframe3 = pd.read_csv(join(path_prefix, filename_third_data))

dataframe3.head()
#bus stop

dataframe3.describe()

filename_fourth_data = "DOITT_SUBWAY_STATION_01_13SEPT2010.csv"
dataframe4 = pd.read_csv(join(path_prefix, filename_fourth_data))

dataframe4.head()
#subway

dataframe4.tail()

dataframe4.isna().sum()

"""## Exploratory Data Analysis

"""

##Here, using main data we extracted the price and neighbourhood group column. Then compared between.
#NegNames = list(df["neighbourhood_group"].unique())
#PriceList = []
#for i in NegNames:
#  PriceList.append( df[df["neighbourhood_group"] == i]["price"].mean())


ax = df.groupby(by="neighbourhood_group").mean()["price"].plot(kind="bar", color="red")

ax.set_xlabel('neighbourhood groups')
ax.set_ylabel('average price')
ax.set_title('Average prices of neighbourhood groups');

"""<font color = "brown">
<font size = 4px>
First of all we have started our exploaration with looking how average prices of lots vary by neighbourhood groups. As it is seen, most expensive neighbourhood group is Manhattan and the cheapest is Bronx.
</font>
"""

ax=df.groupby(by="neighbourhood_group")["room_type"].count().plot(kind="bar", color="orange")

ax.set_xlabel('neighbourhood_group')
ax.set_ylabel('number of lots')
ax.set_title('The number of lots');

"""<font color = "brown">
<font size = 4px>
In this figure, it is seen that there are by far more lots than the Bronx in Manhattan. 
</font>
"""

data = list(df.groupby('room_type')['price'].mean()) # Average price by room type
plt.figure(figsize=(10,5))
plt.bar(df["room_type"].unique(), data)
plt.title("Average prices ($) by Room Type")
plt.xlabel("Room Type")
plt.ylabel("Average prices ($)")
plt.show()

#This is a list of average price by room type. 
print('Average price by room type:')
showLotPrice = list(df.groupby('room_type')['price'].mean()) # Average price by room type
print(showLotPrice)

"""<font color = "brown">
<font size = 4px>
In this figure, it is seen that there private room is the most expensive type of lot among entire home and shared room.
</font>
"""

ManhattanNeigh = list(df[df["neighbourhood_group"]== "Manhattan"]["neighbourhood"].unique())
neigNames=[]
neigPrc=[]
for j in ManhattanNeigh:
  neigNames.append(j)
  neigPrc.append(df[(df["neighbourhood_group"] == "Manhattan") & (df["neighbourhood"] == j)]["price"].mean())

plt.figure(figsize=(25,15))
plt.barh(neigNames, neigPrc)
plt.title("Neighbourhoods in Manhattan")
plt.xlabel("Average prices ($)")
plt.show()

"""<font color = "brown">
<font size = 4px>
Here, we looked Manhattan's neighbourhoods individually by average price. To do so, we got some insight about average prices.
</font>
"""

BronxNeigh = list(df[df["neighbourhood_group"]== "Bronx"]["neighbourhood"].unique())
neigNamesQ=[]
neigPrcQ=[]
for j in BronxNeigh:
  neigNamesQ.append(j)
  neigPrcQ.append(df[(df["neighbourhood_group"] == "Bronx") & (df["neighbourhood"] == j)]["price"].mean())
plt.figure(figsize=(25,15))
plt.barh(neigNamesQ, neigPrcQ)
plt.title("Bronx Neighbourhoods")
plt.xlabel("Average prices")
plt.show()

"""<font color = "brown">
<font size = 4px>
We have done same thing for Bronx. We looked at the neighbourhoods average price. According to Bronx neighbourhoods graph, amoung the neighbourhoods Riverdale by far the most expensive neighbourhood in Bronx. This one should be analyze deeply later.
</font>
"""

numofPoiManhattan =0
numofPoiBronx = 0
for j in (dataframe2["Borough"]):
  if j == "Manhattan":
    numofPoiManhattan+= 1
  if j == "Bronx":
    numofPoiBronx+= 1

plt.bar( ['Manhattan','Bronx'], [numofPoiManhattan, numofPoiBronx])
plt.title("Number of POI's Difference")
plt.ylabel("The amount of POI's ")
plt.show()

"""<font color = "brown">
<font size = 4px>
After understood the most expensive and cheapest neighbourhood groups as Manhattan and Bronx, now we looked at the amount of the POIs effect neighbourhoods prices using our external source which is "Areas_of_Interest_Centroids.csv.'). 
</font>
<br>
<br>
<font color = "brown">
<font size = 4px>
It is seen that, Manhattan has 13 POIs and Bronx has 15. Although Manhattan is more expensive than Bronx, it is clearly seen that the amount of POIs has no affect on price differences.
</font>
"""

numofSubwayManhattan =0
numofSubwayBronx = 0
for j in (dataframe3["BoroName"]):
  if j == "Manhattan":
    numofSubwayManhattan+= 1
  if j == "Bronx":
    numofSubwayBronx+= 1

plt.bar( ['Manhattan','Bronx'], [numofSubwayManhattan, numofSubwayBronx])
plt.title("Number of Bus Stop Differences")
plt.ylabel("The amount of Bus Stops ")
plt.show()

"""<font color = "brown">
<font size = 4px>
This time, we analyse number of bus stops in individual neighbourhoods and its affect on lot prices
</font>
<br>
<br>
<font color = "brown">
<font size = 4px>
It is seen that, Manhattan have over 700 Bus Stops and Bronx have approximately 500. ın this case, the quantity of bus stops might have an effect on high prices in Manhattan.
</font>
"""

lotCountM = list(df[(df["neighbourhood_group"] == "Manhattan")].groupby(["room_type"])["id"].count())
ManhattanRooms = list(df[df["neighbourhood_group"]== "Manhattan"]["room_type"].unique())

print(lotCountM)
print(ManhattanRooms)

plt.bar(ManhattanRooms, lotCountM)
plt.title("Manhattan Lot Types")
plt.xlabel("Room Types")
plt.ylabel("Number of Lots")
plt.show()

"""<font color = "brown">
<font size = 4px>
In this figure we are able to see how many hosts (lot) exist in different types in Manhattan
</font>

"""

lotCountB = list(df[(df["neighbourhood_group"] == "Bronx")].groupby(["room_type"])["id"].count())
BronxRooms = list(df[df["neighbourhood_group"]== "Bronx"]["room_type"].unique())

print(lotCountB)
print(ManhattanRooms)

plt.bar(['Entire home/apt', 'Private room', 'Shared room'], [379, 652, 60])
plt.title("Bronx Lot Types")
plt.xlabel("Room Types")
plt.ylabel("Number of Lots")
plt.show()

"""<font color = "brown">
<font size = 4px>
In this figure we are able to see how many hosts (lot) exist in different types in Bronx
</font>
"""

aveLotPriceM = list(df[(df["neighbourhood_group"] == "Manhattan")].groupby(["room_type"])["price"].mean())
BronxRooms = list(df[df["neighbourhood_group"]== "Manhattan"]["room_type"].unique())

print(aveLotPriceM)
print(BronxRooms)

plt.bar(['Private room', 'Entire home/apt', 'Shared room'], aveLotPriceM)
plt.title("Manhattan lot types")
plt.xlabel("Room Types")
plt.ylabel("Average price")
plt.show()

"""<font color = "brown">
<font size = 4px>
In this figure we are able to see average prices in different lot types in Manhattan.
</font>
"""

aveLotPriceB = list(df[(df["neighbourhood_group"] == "Bronx")].groupby(["room_type"])["price"].mean())
BronxRooms = list(df[df["neighbourhood_group"]== "Bronx"]["room_type"].unique())

print(aveLotPriceB)
print(BronxRooms)

plt.bar(BronxRooms, aveLotPriceB)
plt.title("Bronx lot types")
plt.xlabel("Room Types")
plt.ylabel("Average price")
plt.show()

"""

---


<font color = "brown">
<font size = 4px>
In this figure we are able to see average prices in different lot types in Bronx.
</font>


---

"""

cloud = WordCloud(background_color = "white", max_words = 200)
wordcloud = cloud.generate(str(df['name'].values))
plt.figure(figsize = (15,8))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

"""<font color = "brown">
<font size = 4px>
These words are most commonly used in the titles of the lots in our main data.
</font>

---


## Future Work

<font color="blue">
We have seen that Riverdale is by far the most expensive neighbourhood. We thought there might be some unusal condition for Riverdale In future works, we can analyze the data in detail.

Our subway data will be used in the future when we dive deeply.

---


</font>
"""

#Manhattan's top 5 neighbourhoods
df[df["neighbourhood_group"] == "Manhattan"][["price","neighbourhood"]].sort_values(by="price", ascending=False).head()

#Bronx's top 5 neighbourhoods
df[df["neighbourhood_group"] == "Bronx"][["price","neighbourhood"]].sort_values(by="price", ascending=False).head()

"""---


#Hypothesis Testing


---

### Sample Hypothesis Test 1 

**Hypothesis Test:**
We want to test whether there is significant differences in terms of number of reviews for different neighborhood.


**Null Hypothesis ($H_0$)**: Means of number of reviews for all different neighborhood are same (e.g.  $rewMan$  denotes Manhattan neighborhood)

$ H_0: \mu_{rewBrx} = \mu_{rewMan} = \mu_{rewSta} = \mu_{rewByn} = \mu_{rewQue}$

**Alternative Hypothesis ($H_A$)**: Means of number of reviews for all target values are not same 

$H_A$: means $\mu_{rewBrx}, \mu_{rewMan}, \mu_{rewSta}, \mu_{rewByn},\mu_{rewQue}$ are not same.

**Significance level**: As most of hypothesis tests assume significance level as `0.05`, we are setting it as `0.05` for our test too.
"""

sample_Brooklyn,sample_Manhattan,sample_Queens,sample_StatenIsland,sample_Bronx = [df[df['neighbourhood_group'] == i] for i in df.neighbourhood_group.unique()]

fig, ax = plt.subplots(2, 3, figsize=(25,10)) 
        
sample_Manhattan['number_of_reviews'].plot(kind="hist", ax=ax[0][0], bins=50, label="none", color="c", density=True)
ax[0][0].set_title("Manhattan")

sample_Bronx['number_of_reviews'].plot(kind="hist", ax=ax[0][1], bins=50, label="none", color="m", density=True)
ax[0][1].set_title("Bronx")

sample_Brooklyn['number_of_reviews'].plot(kind="hist", ax=ax[0][2], bins=50, label="none", color="y", density=True)
ax[0][2].set_title("Brooklyn")

sample_Queens['number_of_reviews'].plot(kind="hist", ax=ax[1][0], bins=50, label="none", color="r", density=True)
ax[1][0].set_title("Queens")

sample_StatenIsland['number_of_reviews'].plot(kind="hist", ax=ax[1][1], bins=50, label="none", color="b", density=True)
ax[1][1].set_title("Staten Island")

sns.kdeplot(sample_Manhattan['number_of_reviews'], shade=True, label="Manhattan", ax=ax[1][2], color="c")
sns.kdeplot(sample_Bronx['number_of_reviews'], shade=True, label="Bronx", ax=ax[1][2], color="m")
sns.kdeplot(sample_Brooklyn['number_of_reviews'], shade=True, label="Brooklyn", ax=ax[1][2], color="y")
sns.kdeplot(sample_Queens['number_of_reviews'], shade=True, label="Queens", ax=ax[1][2], color="r")
sns.kdeplot(sample_StatenIsland['number_of_reviews'], shade=True, label="Staten Island", ax=ax[1][2], color="b")
ax[1][2].set_title("Comparison with Neighbourhood Groups")

plt.suptitle("Number of Reviews")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

f_stats, p_values = f_oneway(sample_Manhattan['number_of_reviews'].values, sample_Bronx['number_of_reviews'].values, sample_Brooklyn['number_of_reviews'].values, sample_Queens['number_of_reviews'].values, sample_StatenIsland['number_of_reviews'].values)
p_values

"""**Interpretation of result:**

> As p-value we obtained is smaller than the threshold significance level 0.05, we can conclude that means of number of reviews are not the same for all 5 neighborhoods. **Here, we reject the null hypothesis.**

### Sample Hypothesis Test 2 
**Hypothesis Test:**
We want to test whether there is a  significant difference in terms of average prices for different lot types.


**Null Hypothesis ($H_0$)**: average prices for all lot types are same (e.g.  $lotP$  denotes Private House lot type)

$ H_0: \mu_{lotP} = \mu_{lotE} = \mu_{lotS}$

**Alternative Hypothesis ($H_A$)**: average prices for all lot types are not same (e.g.  $lotP$  denotes Private House lot type)

$ H_A$: means of $\mu_{lotP}, \mu_{lotE}, \mu_{lotS}$ are not same.

**Significance level**: As most of hypothesis tests assume significance level as `0.05`, we are setting it as `0.05` for our test too.
"""

priv,ent,sha = [df[df['room_type'] == i] for i in df.room_type.unique()]

fig, ax = plt.subplots(2, 2, figsize=(25,10)) 
        
priv['price'].plot(kind="hist", ax=ax[0][0], bins=50, label="Private room", color="c", density=True)
ax[0][0].set_title("Private room")

ent['price'].plot(kind="hist", ax=ax[0][1], bins=50, label="Entire house/apt", color="m", density=True)
ax[0][1].set_title("Entire house/apt")

sha['price'].plot(kind="hist", ax=ax[1][0], bins=50, label="Shared room", color="y", density=True)
ax[1][0].set_title("Shared room")

sns.kdeplot(sample_Manhattan['price'], shade=True, label="Private room", ax=ax[1][1], color="c")
sns.kdeplot(sample_Bronx['price'], shade=True, label="Entire house/apt", ax=ax[1][1], color="m")
sns.kdeplot(sample_Brooklyn['price'], shade=True, label="Shared room", ax=ax[1][1], color="y")

ax[1][1].set_title("Comparison with Lot Types")

plt.suptitle("Price")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

f_stats, p_values = f_oneway(priv['price'].values, ent['price'].values, sha['price'].values)
p_values

"""**Interpretation of result:**

> As p-value we obtained is smaller than the threshold significance level 0.05, we can conclude that avarage prices for all lot types are not the same for all neighborhood groups. **Here, we reject the null hypothesis.**

### Sample Hypothesis Test 3   

**Sample 1:** Houses with subway stations within 5 km range

**Sample 2:** Houses with subway stations outside 5 km range

**Hypothesis Test:** We want to determine whether proximity of subway stations's are directly proportional to the average price of that place.

**Null Hypothesis ($H_0$)**: The average price of sample 1 $(\mu_{1})$ is equal to the average price of sample 2 $(\mu_{2})$.

$ H_0: \mu_{1} = \mu_{2}$

**Alternative Hypothesis ($H_A$)**: The average price of sample 1 $(\mu_{1})$ is not equal to the average price of sample 2 $(\mu_{2})$.

$ H_A: \mu_{1} != \mu_{2}$

**Significance level**: As most of hypothesis tests assume significance level as `0.05`, we are setting it as `0.05` for our test too.
"""

latlist = []
lonlist = []
for i in dataframe4["the_geom"].to_list():
  latlist.append(float(i.split()[2][:-1]))
  lonlist.append(float(i.split()[1][1:])) 

dataframe4["longitude"] = lonlist
dataframe4["latitude"] = latlist
dataframe4.head()

from sklearn.neighbors import BallTree
from math import radians

listing_locs = df[["latitude", "longitude"]].values
subway_locs = dataframe4[["latitude", "longitude"]].values
tree = BallTree(subway_locs, metric="haversine")
nearest_distances, nearest_subway_indices = tree.query(listing_locs)
nearest_distances[0]

nearest_distances_km = []
for i in nearest_distances:
  nearest_distances_km.append(i * 6371)
#We multiplied i with 6371 because earth's radius is 6371 km.

df.head()

x = list(df["price"])
y = nearest_distances_km
plt.scatter(y, x)

plt.ylabel("Price ")
plt.xlabel("Distance in km")
plt.show()

"""<font color = "brown">
<font size = 4px>
Scatter plot which indicates the distribution of the distance of subway stations to lots in terms of price
</font>
"""

isLessThan = []
for i in nearest_distances_km:
  if i < 5:
    isLessThan.append(1)
  else:
    isLessThan.append(0)
df["isLessThan5Km"] = isLessThan

test_type = "price"

lessThan = df[df["isLessThan5Km"] == 1][test_type]
greaterThan = df[df["isLessThan5Km"] == 0][test_type]

fig, ax = plt.subplots(1, 3, figsize=(24,6))

lessThan.plot(kind="hist", ax=ax[0], bins=1000, label="lessThan", color="c", density=True)
ax[0].set_title("Within 5 km")
ax[0].set_xlim(0,1000)
ax[0].set(xlabel="Price", ylabel="Frequency")

greaterThan.plot(kind="hist", ax=ax[1], bins=1000, label="greaterThan", color="m", density=True)
ax[1].set_title("Not within 5 km")
ax[1].set_xlim(0,1000)
ax[1].set(xlabel="Price", ylabel="Frequency")

sns.kdeplot(lessThan, shade=True, label="Within 5 km", ax=ax[2], color="c")
sns.kdeplot(greaterThan, shade=True, label="Not within 5 km", ax=ax[2], color="m")

ax[2].set_title("Comparison with KDE")
ax[2].set_xlim(0,1000)
ax[2].set(xlabel="Price", ylabel="Frequency")

plt.suptitle("Distance-price distribution")
plt.show()

"""**T-test:**


"""

stats.ttest_ind(lessThan, greaterThan, equal_var=False)

"""**Interpretation of result:**

> As p-value we obtained is smaller than the threshold significance level 0.05, we can conclude that proximity of subway stations's are not directly proportional to the average price of that place. **Here, we reject the null hypothesis.**

### Sample Hypothesis Test 4

**Sample 1:** Houses with POI's within 25 km range

**Sample 2:** Houses with POI's outside 25 km range

**Hypothesis Test:** We want to determine whether proximity of POI's are directly proportional to the average price of that place.

**Null Hypothesis ($H_0$)**: The average price of sample 1 $(\mu_{1})$ is equal to the average price of sample 2 $(\mu_{2})$.

$ H_0: \mu_{1} = \mu_{2}$

**Alternative Hypothesis ($H_A$)**: The average price of sample 1 $(\mu_{1})$ is not equal to the average price of sample 2 $(\mu_{2})$.

$ H_A: \mu_{1} != \mu_{2}$

**Significance level**: As most of hypothesis tests assume significance level as `0.05`, we are setting it as `0.05` for our test too.
"""

latlistPOI = []
lonlistPOI = []
for i in dataframe2["the_geom"].to_list():
  latlistPOI.append(float(i.split()[2][:-1]))
  lonlistPOI.append(float(i.split()[1][1:])) 

dataframe2["longitude"] = lonlistPOI
dataframe2["latitude"] = latlistPOI
dataframe2

dataframe2.head()

from sklearn.neighbors import BallTree
from math import radians

listing_locs = df[["latitude", "longitude"]].values
POI_locs = dataframe2[["latitude", "longitude"]].values

# radians için bir fonksiyon
#r = np.vectorize(lambda x: radians(x))
# lat-lng değerlerini radians'a çevir
#listing_locs = r(listing_locs)
#subway_locs = r(subway_locs)

# ballTree'ye subway listesini gir
# haversine, yeryüzü üzerindeki noktalar arasındaki mesafeler için
treePOI = BallTree(POI_locs, metric="haversine")

# oluşan tree'den listing loc'ları için en yakın noktaları çek
nearest_distancesPOI, nearest_POI_indices = treePOI.query(listing_locs)

#Bunlar distance matrix değil. Her iki variable da (nearest_distances, nearest_subway_indices) listing_locs ile aynı length'e sahip. Aynı index'tekiler de aynı objeler. Örneğin ilk airbnb listing'in nearest distance ve hangi subway durağı olduğunu anlamak için

nearest_distancesPOI[0]  # en yakındaki durağın mesafesi
#subway_locs[nearest_subway_indices[0]] # durağın lat-lng değerleri

#Yani nearest_subway_indices orijinal subway_locs'taki değerlerin index'lerini saklıyor. Bu iki array'i istediğin gibi manipüle edebilirsin.

nearest_distances_km_POI = []
for i in nearest_distancesPOI:
  nearest_distances_km_POI.append(i * 6371)
#We multiplied i with 6371 because earth's radius is 6371 km.

x = list(df["price"])
y = nearest_distances_km_POI
plt.scatter(y, x)
plt.ylabel("Price ")
plt.xlabel("Distance in km")
plt.show()

isLessThanPOI = []
for i in nearest_distances_km_POI:
  if i < 25:
    isLessThanPOI.append(1)
  else:
    isLessThanPOI.append(0)
df["isLessThan10KmPOI"] = isLessThanPOI

test_type = "price"

lessThanPOI = df[df["isLessThan10KmPOI"] == 1][test_type]
greaterThanPOI = df[df["isLessThan10KmPOI"] == 0][test_type]

fig, ax = plt.subplots(1, 3, figsize=(24,6))

lessThanPOI.plot(kind="hist", ax=ax[0], bins=1000, label="Within 25 km", color="c", density=True)
ax[0].set_title("Within 25 km")
ax[0].set_xlim(0,1000)

greaterThanPOI.plot(kind="hist", ax=ax[1], bins=1000, label="Not within 25 km", color="m", density=True)
ax[1].set_title("Not within 25 km")
ax[1].set_xlim(0,1000)

sns.kdeplot(lessThanPOI, shade=True, label="completed students", ax=ax[2], color="c")
sns.kdeplot(greaterThanPOI, shade=True, label="none students", ax=ax[2], color="m")
ax[2].set_title("Comparison with KDE")
ax[2].set_xlim(0,1000)

plt.suptitle("Distance-price Distributions")
plt.show()

"""**T-test:**"""

stats.ttest_ind(lessThanPOI, greaterThanPOI, equal_var=False)

"""**Interpretation of result:**

> As p-value we obtained is smaller than the threshold significance level 0.05, we can conclude proximity of POI's are not directly proportional to the average price of that place. **Here, we reject the null hypothesis.**

### Sample Hypothesis Test 5

**Sample 1:** Houses within 5 km from the subway stations.

**Sample 2:** Houses within 5 km from the bus stops.

**Hypothesis Test:** We aimed to compare the average prices of houses with a bus stop within 5 km and a metro stop within 5 km.

**Null Hypothesis ($H_0$)**: The average price of sample 1 $(\mu_{1})$ is equal to the average price of sample 2 $(\mu_{2})$.

$ H_0: \mu_{1} = \mu_{2}$

**Alternative Hypothesis ($H_A$)**: The average price of sample 1 $(\mu_{1})$ is not equal to the average price of sample 2 $(\mu_{2})$.

$ H_A: \mu_{1} != \mu_{2}$

**Significance level**: As most of hypothesis tests assume significance level as `0.05`, we are setting it as `0.05` for our test too.
"""

dataframe3.head()

from sklearn.neighbors import BallTree
from math import radians

listing_locs = df[["latitude", "longitude"]].values
bus_locs = dataframe3[["LATITUDE", "LONGITUDE"]].values

# radians için bir fonksiyon
#r = np.vectorize(lambda x: radians(x))
# lat-lng değerlerini radians'a çevir
#listing_locs = r(listing_locs)
#subway_locs = r(subway_locs)

# ballTree'ye subway listesini gir
# haversine, yeryüzü üzerindeki noktalar arasındaki mesafeler için
treeBus = BallTree(bus_locs, metric="haversine")

# oluşan tree'den listing loc'ları için en yakın noktaları çek
nearest_distancesBus, nearest_bus_indices = treeBus.query(listing_locs)

#Bunlar distance matrix değil. Her iki variable da (nearest_distances, nearest_subway_indices) listing_locs ile aynı length'e sahip. Aynı index'tekiler de aynı objeler. Örneğin ilk airbnb listing'in nearest distance ve hangi subway durağı olduğunu anlamak için

nearest_distancesBus[0]  # en yakındaki durağın mesafesi
#subway_locs[nearest_subway_indices[0]] # durağın lat-lng değerleri

#Yani nearest_subway_indices orijinal subway_locs'taki değerlerin index'lerini saklıyor. Bu iki array'i istediğin gibi manipüle edebilirsin.

nearest_distances_Bus_km = []
for i in nearest_distancesBus:
  nearest_distances_Bus_km.append(i * 6371)
#We multiplied i with 6371 because earth's radius is 6371 km.

df.head()



isLessThanBus = []
for i in nearest_distances_Bus_km:
  if i <5:
    isLessThanBus.append(1)
  else:  
    isLessThanBus.append(0)

df["isLessThan5KmBus"] = isLessThanBus

test_type = "price"

lessThanBus = df[df["isLessThan5KmBus"] == 1][test_type]
lessThan = df[df["isLessThan5Km"] == 1][test_type]

fig, ax = plt.subplots(1, 3, figsize=(24,6))

lessThanBus.plot(kind="hist", ax=ax[0], bins=1000, label="Within 5 km in Bus stops", color="c", density=True)
ax[0].set_title("Within 5 km in BUS")
ax[0].set_xlim(0,1000)

lessThan.plot(kind="hist", ax=ax[1], bins=1000, label="lessThan", color="m", density=True)
ax[1].set_title("Within 5 km SUBWAY")
ax[1].set_xlim(0,1000)
ax[1].set(xlabel="Price", ylabel="Frequency")

sns.kdeplot(lessThanBus, shade=True, label="completed students", ax=ax[2], color="c")
sns.kdeplot(lessThan, shade=True, label="none students", ax=ax[2], color="m")
ax[2].set_title("Comparison with KDE")
ax[2].set_xlim(0,1000)

plt.suptitle("Distance-price Distributions")
plt.show()

stats.ttest_ind(lessThanBus, lessThan, equal_var=False)

"""**Interpretation of result:**

> As p-value we obtained is smaller than the threshold significance level 0.05, we can conclude proximity of POI's are not directly proportional to the average price of that place. **Here, we reject the null hypothesis.**

**IMPORTANT NOTE:** This must be noted that p-value is extremely close to the significance level 0.05.

---
# Machine Learning
---
"""

from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from os.path import join
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import r2_score
from sklearn.tree import DecisionTreeRegressor

"""### Reggression Model -1: Lineer Regression

"""

df.head()

df.dropna(subset = ["reviews_per_month"], inplace=True)
df.head()

"""

>By doing so, we excluded the NaN values in the reviews per month column."""

dfcopy = df
from sklearn.preprocessing import OrdinalEncoder
encoder = OrdinalEncoder()
encoded_input = encoder.fit_transform(np.array(list(dfcopy["room_type"])).reshape(-1,1))
dfcopy["room_type"] = encoded_input
encoded_input

encoded_input_2 = encoder.fit_transform(np.array(list(dfcopy["neighbourhood_group"])).reshape(-1,1))
dfcopy["neighbourhood_group"] = encoded_input_2
encoded_input_2

"""

>In our main data, room type and neighbourhood group columns do not fit into the machine learning crtierias due to its non-numerical nature. So, we encoded to put these non-numerical values into numbers such as 1,2,3 etc.

Additionaly, we included room type and neighbourhood group in order to decrease the mean absolute error. Before we include these features mean absolute error was 85. After inclusion, it becomes nearly 65. """

features = ["neighbourhood_group","room_type","number_of_reviews","minimum_nights","availability_365","reviews_per_month","calculated_host_listings_count","isLessThan5Km", "isLessThan10KmPOI", "isLessThan5KmBus"]
X = dfcopy[features].values  # converting to column vector
target = dfcopy['price'].values

# train-test split
X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.1, random_state=0)

"""Here important note: We have add 3 features from different data sets namely isLessThan5Km, isLessThan10KmPOI, isLessThan5KmBus"""

# creating the model
model = LinearRegression()  
model.fit(X_train, y_train)

# let's have a look at the coefficients, 
coeff_df = pd.DataFrame(model.coef_, features, columns=['Coefficient'])  
coeff_df

"""

>We found coefficients individually."""

from sklearn.metrics import mean_squared_error, mean_absolute_error

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mse)

print("mse: {}".format(mse))
print("mae: {}".format(mae))
print("rmse: {}".format(rmse))
print(r2_score(y_test, y_pred))

"""**Interpretation of result:**

>We conclude that the analyized features do not correlate strongly with the price. Therefore, the high mean absolute error can be explanid with this argument. 

>The r^2 value is close to 0.11 which demonstrates that they are weakly correlated in positive direction.

### Reggression Model - 2: Decision Tree
"""

from sklearn.tree import DecisionTreeRegressor

x_ml = dfcopy[["neighbourhood_group","room_type","minimum_nights","number_of_reviews","reviews_per_month","calculated_host_listings_count","availability_365","isLessThan5Km", "isLessThan10KmPOI", "isLessThan5KmBus"]]
y_ml = dfcopy[["price"]]
x_train_ml, x_test_ml, y_train_ml, y_test_ml = train_test_split(x_ml, y_ml, test_size = 0.1, random_state = 42)

tree_model = DecisionTreeRegressor()
tree_model.fit(x_train_ml, y_train_ml)
y_predict_tree_model = tree_model.predict(x_test_ml)
r2_score(y_test_ml["price"], y_predict_tree_model)

mean_absolute_error(y_test_ml["price"], y_predict_tree_model)

"""**Interpretation of result:**

>In contrast to lineer regression above, we found that the analyzed features  correlates strongly with the price in negative direciton. However, the mean absolute error is higher than the lineer regression model's.

### Regularization Methods for Regression Models - 1: RIDGE
"""

from sklearn.linear_model import Ridge

ridgeRegression = Ridge(alpha = 0.001)
ridgeRegression.fit(x_train_ml, y_train_ml)
y_predict_ridge_model = ridgeRegression.predict(x_test_ml)
r2_score(y_test_ml["price"], y_predict_ridge_model)
#mean_absolute_error(y_test_ml["price"], y_predict_ridge_model)

ridgeRegression = Ridge(alpha = 10000)
ridgeRegression.fit(x_train_ml, y_train_ml)
y_predict_ridge_model = ridgeRegression.predict(x_test_ml)
r2_score(y_test_ml["price"], y_predict_ridge_model)
#mean_absolute_error(y_test_ml["price"], y_predict_ridge_model)

"""**Interpretation of result:**

>Ridge and Lasso regression are some of the simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression. 

The regularization model Ridge suggest that higher the alpha value, more restriction on the coefficients; low alpha means more generalization. 

When the coefficients are less restricted, the magnitudes of the coefficients are almost same as of linear regression. For higher value of α (10000), the magnitudes are considerably less compared to linear regression case. This is an example of shrinking coefficient magnitude using Ridge regression.

### Regularization Methods for Regression Models - 2: LASSO
"""

from sklearn.linear_model import Lasso

lassoRegression = Lasso(alpha = 100)
lassoRegression.fit(x_train_ml, y_train_ml)
y_predict_lasso_model = lassoRegression.predict(x_test_ml)
r2_score(y_test_ml["price"], y_predict_lasso_model)
#print(mean_absolute_error(y_test_ml["price"], y_predict_lasso_model))

lassoRegression = Lasso(alpha = 0.001)
lassoRegression.fit(x_train_ml, y_train_ml)
y_predict_lasso_model = lassoRegression.predict(x_test_ml)
r2_score(y_test_ml["price"], y_predict_lasso_model)
#print(mean_absolute_error(y_test_ml["price"], y_predict_lasso_model))

"""**Interpretation of result:**

>Lasso regression not only helps in reducing over-fitting but it can help us in feature selection.

When we increase alpha value, r2 scores approximates to the negative value.

### Random Forest Model -  Scaling/Normalization and Hyper-parameter Tuning
"""

from sklearn.ensemble import RandomForestRegressor

rf_regressor = RandomForestRegressor()
rf_regressor.fit(x_train_ml, y_train_ml["price"])
y_predict_rf_regressor = rf_regressor.predict(x_test_ml)
r2_score(y_test_ml["price"], y_predict_rf_regressor)

mean_absolute_error(y_test_ml["price"], y_predict_rf_regressor)

train_set_accuracies = []
val_set_accuracies = []
errors = []

for depth in range(50):
  model = DecisionTreeRegressor(max_depth = depth + 1)
  model.fit(x_train_ml, y_train_ml)
  train_preds = model.predict(x_train_ml)
  val_preds = model.predict(x_test_ml)
  train_set_accuracies.append(r2_score(y_train_ml, train_preds))
  val_set_accuracies.append(r2_score(y_test_ml, val_preds))
  errors.append(mean_absolute_error(y_test_ml, val_preds))

errors

"""We built Random Forest model with varying values for the max_depth hyper-parameter. As the name of it also suggests, this parameter controls the maximum depth that the Random Forest can grow.

# Conclusion

To sum up everything that has been stated so far, We started to analyze our main data by comparing different neighbourhood groups in terms of lot prices. After that we investigate whether any features from our datasets would have an effect on the price. After that we stated 5 hypothesis regarding the features such as number of reviews, lot types, transportation availabilities, POI and etc. Finally, we put those different features into the various machine learning techniques. As it is seen above, we can only predict price with the +-65$ price change. These mean absolute errors show that our predictions are not as good as we expected. This might be caused by the weak correlation between our features and price.
"""